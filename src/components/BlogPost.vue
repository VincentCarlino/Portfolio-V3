<template>
  <div class="bg blog-bg">
    <div class="page-wrapper">
        <div class="content">
            <div class="blog-title context-note">
                    <p><strong>Context note: </strong>It’s popular within the community of web developers and designers to publish a portfolio website to showcase previously completed projects and work.  Often, these websites will function simultaneously as a blog for the creator to express thoughts about things within the field of software.  This document is intended to be a blog post on my personal portfolio website (oh wait...it actually is) as this is an important topic within the field of web development.  This article is intended for an audience of internet users that are active within online communities, meaning they regularly partake in online discussions and are likely to witness an example of hatred in their own community.  They may be aware of the problems that are occurring as a result of relaxed content regulation online, but they do not understand the variety of ways the problem has been/could be addressed.  This article is meant to bring awareness to the currently dangerous state of online content regulation, show how certain methods of regulation have failed, and begin a discussion within this audience to drive policy changes forward.</p>
            </div>
            <div class="blog-title">
                <div>
                    <h1>How the Web Handles Hate and How It Can Change</h1>
                    <h5>by Vincent Carlino</h5>
                </div>
                <img src="/static/Computer.png" alt="">
            </div>
            <div class="blog-content">
                <p><huge>O</huge>n March 15, 2019, nearly 100 people at two mosques in Christchurch, New Zealand were killed or injured by the hand of an armed attacker.  The shooter, a self identified member of the alt-right, took his attack to an audience on Facebook Live.  Tragedies as a result of hate crimes like this have seen a frightening increase in frequency during the last year.  The online presence of the alt-right and other violent groups have also seen a recent boom of growth as more people become comfortable in online communities.   With more people venturing onto the web, hateful communities are given the opportunity to expose unsuspecting users to their ways of thought.  The Christchurch shooter’s use of Facebook in this attack is a great (for lack of a better word) example of the recent trend of violent groups using the internet to intentionally disseminate hate.  Members of hateful groups are able to easily penetrate communities on all kinds of online platforms, and the current policies for online content regulation often allow this kind of dissemination to go unaddressed. </p>
                <p>This means hate groups will continue to grow as long as they are given a large public forum to voice their opinions.  Tech companies irregularly take responsibility in correcting negative social change that occurs due to their platforms and the internet is just too socially powerful for that kind of attitude to be acceptable.  In order for further tragedies to be prevented and hate group growth to cease, changes need to be made in how the internet’s main actors handle content that promotes the dissemination of hate.  As active users of the internet, we need to first step back and evaluate how today’s content regulation practices are unsustainable in preventing hate groups from finding success online, then begin to discuss the options for us (as users) and tech companies to make changes.</p>
                <h3>So why aren't companies doing more in the first place?</h3>
                <p>Tech companies have reasons for why they have made few significant attempts to regulate hate, the first of which is that interfering too much in online discourse might lead to users feeling cheated out of the freedom they expect while online. The feeling of being controlled could cause some users to leave a particular service in protest, leading to a loss of business for that tech company.  The second, more potent reason for this apathy is the law in the  United States that grants companies the choice over whether they take want to ownership/responsibility for content on their platform.  <a href="https://www.minclaw.com/legal-resource-center/what-is-section-230-of-the-communication-decency-act-cda/" target="blank">Section 230</a> of the Communications Decency Act states that internet service providers shall not be treated as the owner of any content produced by a user of the service.  Protections like those of Section 230 keep the legal responsibility for content posted online out of the hands of these companies and (dangerously) into the hands of anyone with access to an internet connection.   Tech companies have essentially provided a giant public chalkboard for people to draw on.  While that chalkboard is full of beautiful artwork and discussion, it’s also full of phallic imagery and defamations that <italic>should</italic> probably be erased by the people that put the chalkboard there in the first place.  Nevertheless, the role tech companies play in shaping society in a digital age is far too important for them to not take responsibility for what goes on in their spaces.  Legal measures to combat hate are often inconsistent in eliciting a significantly positive outcome, so the potential for any change to happen on the web ultimately lies with the companies that can exercise control over online policy.  </p>
                <h3>What could tech companies do?</h3>
                <p>An obvious solution is to modify the existing methods of regulation to be more restricting about what content is deemed inappropriate for general users of a site.  By exercising a tighter grasp on the content of their platforms, tech companies could make a direct difference in the probability of a user becoming exposed to something hurtful.  This change could be done simply through an update to the website’s terms and conditions of service.  By changing the rules users are asked to follow, companies can more easily find a reason for some content to be removed.  This change might also be done technologically, where the software developers of a given social media network could make adjustments to how content is <italic>automatically</italic> moderated.  Algorithms that scan the content we post for offensive, violent, or dangerous material could be modified to be more strict.  Such a change would effectively squash hateful material at its source by preventing it from reaching other users altogether.  Still, a more rigorous filtering algorithm would only work under ideal conditions since filtering algorithms like these, as impressively human as they might seem, will never judge with the same subjectivity as a real person.  And as much as I loved The Matrix, I see some issues in the concept of a computer exercising control over how humanity grows and develops.  In order to preserve the sanctity of humanity in cyberspace and ultimately keep people safe, we need an equally human force to determine what is right from wrong.  </p>
                <p>Such a human force already exists in the form of special staff members for tech companies.  Facebook employs some people to their Community Operations team as Content Moderators.  Content Moderators serve the Facebook community as a whole by manually filtering what goes on the platform.  Staffed moderators need to manage all content that goes on the platform, which means they may unfortunately be exercising some control over communities they aren’t invested in. Additionally, the job function of a content moderator creates a scenario where these employees are constantly bombarded by material that could be detrimental for their mental health.  They spend all day looking for posts that contain offensive language, horrifying violence, and illegal pornography.  The actual effects of this position on these employees has been investigated previously by <a target="blank" href="https://www.theverge.com/2019/2/25/18229714/cognizant-facebook-content-moderator-interviews-trauma-working-conditions-arizona">The Verge</a> , who found that content moderators often left Facebook experiencing PTSD-like symptoms and sometimes even adopting the more hateful opinions they had to search out.</p>
                <h3>What can we as users do?</h3>
                <p>What is needed is a group of people that are personally invested in the good communities of the web that can do this kind of content filtering on a part-time basis.  Such a service already exists in most popular online spaces and is often carried out by the users that care the most about their community.  Facebook has groups, which are all maintained by 1 or more admins.  Reddit has moderators, or “mods” for every subreddit on the platform that serve roughly the same purpose.  Users of social networks have been giving back to their communities for years, and it seems like a much more healthy way of navigating dangerous material without making a full-time job out of it.  Moderators work on their own time and are able to keep the best interest of their own community at heart when faced with a difficult decision.  There is no legal contract keeping them from taking a break, so the mental health risks of a staffed moderator are less likely to cause any harm.  I’m not suggesting companies to do away with their staffed moderators.  They are necessary in the case of managing some well-hidden communities  dedicated to hate or violence where an internal moderator would serve little purpose.</p>
                <p>There are obviously a lot of ways to go about this growing issue, but the best solutions involve building upon the sustainable systems of moderation we already have.  Tech companies and their staffed moderators could reach out to internal moderators to narrow the scope of what their users need in a content regulation policy.  Companies can try to lighten the load on their debilitated staff, and internal moderators can reach out to their communities to expand their moderating teams.  Finally, general internet users like you or I can contribute by becoming a moderator or by acting on dangerous content when it comes up.  Instead of sitting idly on the web as it becomes consumed by hate, we can work towards improving the overall health of the internet by reporting more frequently, keeping our online friends from adopting opinions of hate, and even contacting .  The most common citizens of cyber society have potential to make a difference in how things play out in the future so why shouldn’t we take advantage of that?</p>

                <h5></h5>
            </div>
            <div class="works-cited">
                <h3>Works cited</h3>
                <p>Newton, C. (2019, Feb 25). The secret lives of Facebook moderators in America. Retrieved from <a target="blank" href="https://www.theverge.com/2019/2/25/18229714/cognizant-facebook-content-moderator-interviews-trauma-working-conditions-arizona">https://www.theverge.com/2019/2/25/18229714/cognizant-facebook-content-moderator-interviews-trauma-working-conditions-arizona</a></p>
                <p>Minc Law. What is Section 230 of the Communication Decency Act (CDA)? Retreived from <a href="https://www.minclaw.com/legal-resource-center/what-is-section-230-of-the-communication-decency-act-cda/" target="blank">https://www.minclaw.com/legal-resource-center/what-is-section-230-of-the-communication-decency-act-cda/</a></p>
            </div>
        </div>
    </div>
  </div>
</template>

<script>
export default {
  name: 'Blog',
  data () {
    return {

    }
  }
}
</script>

<!-- Add "scoped" attribute to limit CSS to this component only -->
<style lang="scss" scoped>

.blog-title {
  display: flex;
  align-items: center;
  height: 100vh;
  min-height: 350px;

  img {
      max-width: 50%;
  }

  div {
      text-align: right;
  }
}

a {
    text-decoration: underline;
    color: #2545AF;
}

.works-cited {
    padding: 80px 0;
    text-align: left;
}
.blog-content {
    padding: 0 20%;
}

h1, h3 {
    color: #2545AF;
}

p {
    font-size: 18px;
    line-height: 35px;
    padding-bottom: 45px;
}

italic {
    font-style: italic;
}

huge {
    font-size: 50px;
    font-weight: bold;
}

</style>
